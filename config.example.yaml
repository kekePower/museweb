# MuseWeb Configuration

server:
  address: "127.0.0.1"
  port: "8000"
  prompts_dir: "./prompts"
  # Enable debug mode to see detailed HTTP request/response logs (true/false)
  debug: false

model:
  # The AI backend to use ('ollama' or 'openai')
  backend: "openai"
  # The model name to use for the selected backend
  name: "gpt-4.1-nano"
  # List of model name patterns that support reasoning/thinking tags
  # These patterns are checked in order (first match wins)
  reasoning_models:
    - "deepseek-r1-distill"  # DeepSeek R1 distilled models (most specific first)
    - "mercury-coder"        # Inception Labs Mercury Coder models
    - "mercury"              # Inception Labs Mercury models
    - "sonar-reasoning-pro"  # Perplexity Sonar reasoning pro models
    - "sonar-reasoning"      # Perplexity Sonar reasoning models
    - "gemini-2.5-flash-lite-preview-06-17"  # Specific Gemini model
    - "gemini-2.5-flash"     # Gemini 2.5 Flash models
    - "r1-1776"              # OpenAI R1-1776 models
    - "qwen3"                # Qwen3 models (specific)
    - "deepseek"             # DeepSeek models (general, after specific)
    - "qwen"                 # Qwen models (general, after specific)

openai:
  # Your OpenAI API key. Can be left blank if using the OPENAI_API_KEY environment variable.
  api_key: ""
  # The base URL for the OpenAI API. Useful for local models like LM Studio.
  api_base: "http://api.openai.com/v1"

ollama:
  # Your Ollama API key. Can be left blank if using the OLLAMA_API_KEY environment variable.
  api_key: ""
  # Base URL for your local Ollama server.
  api_base: "http://localhost:11434"
